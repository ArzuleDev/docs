---
title: "Quickstart"
description: "Get traces flowing from your AI agents in under 5 minutes"
---

This guide walks you through installing the Arzule SDK and capturing your first traces from your agent application.

## Prerequisites

- Python 3.10 or higher
- An agent framework (Claude Code, CrewAI, or LangGraph)
- An Arzule account with API credentials

## Step 1: Install the SDK

<CodeGroup>
```bash pip
pip install arzule-ingest
```

```bash poetry
poetry add arzule-ingest
```

```bash uv
uv add arzule-ingest
```
</CodeGroup>

<Note>
The core package includes Claude Code support. For CrewAI or LangGraph, add the extras: `pip install arzule-ingest[crewai]` or `pip install arzule-ingest[langchain]`
</Note>

### Upgrading

To upgrade to the latest version:

<CodeGroup>
```bash pip
pip install --upgrade arzule-ingest
```

```bash poetry
poetry update arzule-ingest
```

```bash uv
uv add --upgrade arzule-ingest
```
</CodeGroup>

## Step 2: Set environment variables

Configure your Arzule credentials. You can find these in your [Arzule dashboard settings](https://app.arzule.com/dashboard/settings).

```bash
export ARZULE_API_KEY="your-api-key"
export ARZULE_TENANT_ID="your-tenant-id"
export ARZULE_PROJECT_ID="your-project-id"
```

## Step 3: Initialize and run

<Tabs>
  <Tab title="Claude Code">
    For Claude Code, use the CLI to configure and install hooks:

    ```bash
    # Configure your credentials (interactive)
    arzule configure

    # Install hooks into Claude Code
    arzule claude install
    ```

    That's it. Use Claude Code normally and traces are captured automatically with each turn.

    <Tip>
    **Tip:** Run `arzule claude status` to verify hooks are installed correctly.
    </Tip>
  </Tab>
  <Tab title="CrewAI">
    First, install with CrewAI support: `pip install arzule-ingest[crewai]`

    Then add two lines to your `main.py` file - import and init:

    ```python {3,4,21}
    #!/usr/bin/env python
    # src/research_crew/main.py
    import arzule_ingest
    arzule_ingest.init()

    import os
    from research_crew.crew import ResearchCrew

    os.makedirs('output', exist_ok=True)

    def run():
        inputs = {
            'topic': 'Artificial Intelligence in Healthcare'
        }

        # Traces are captured automatically
        result = ResearchCrew().crew().kickoff(inputs=inputs)

        print("\n\n=== FINAL REPORT ===\n\n")
        print(result.raw)
        arzule_ingest.shutdown()

    if __name__ == "__main__":
        run()
    ```

    That's it - run `crewai run` as usual and traces flow to Arzule automatically.

    <Tip>
    **Tip:** Setting `allow_delegation=True` on your agents produces cleaner traces and better visualizations. When agents can delegate tasks to each other, Arzule captures these handoffs clearly, giving you a much more readable graph of agent interactions.
    </Tip>
  </Tab>
  {/* <Tab title="LangChain">
    Add two lines at the top of your script - import and get the callback handler:

    ```python {1,2,12,14}
    import arzule_ingest
    handler = arzule_ingest.langchain.instrument_langchain()

    from langchain_openai import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate

    llm = ChatOpenAI(model="gpt-4")
    prompt = ChatPromptTemplate.from_template("Tell me about {topic}")
    chain = prompt | llm

    # Pass the handler to capture traces
    result = chain.invoke({"topic": "AI safety"}, config={"callbacks": [handler]})

    arzule_ingest.shutdown()
    ```
  </Tab> */}
  <Tab title="LangGraph">
    First, install with LangChain support: `pip install arzule-ingest[langchain]`

    Then add two lines at the top of your script - import and get the callback handler:

    ```python {1,2,22,24}
    import arzule_ingest
    handler = arzule_ingest.langgraph.instrument_langgraph()

    from langgraph.graph import StateGraph
    from langchain_openai import ChatOpenAI
    from typing import TypedDict

    class State(TypedDict):
        messages: list
        response: str

    llm = ChatOpenAI(model="gpt-4")

    def chatbot(state: State) -> dict:
        response = llm.invoke(state["messages"])
        return {"response": response.content}

    graph = StateGraph(State)
    graph.add_node("chatbot", chatbot)
    graph.add_edge("__start__", "chatbot")
    compiled = graph.compile()

    # Pass the handler to capture traces
    result = compiled.invoke({"messages": [("user", "Hello!")]}, config={"callbacks": [handler]})

    arzule_ingest.shutdown()
    ```

    <Tip>
    **Tip:** For multi-agent graphs, use `Command(goto="agent_name")` or state-based routing with `next_agent` to get automatic handoff tracking between agents.
    </Tip>
  </Tab>
  {/* <Tab title="AutoGen">
    Add two lines at the top of your script - import and instrument:

    ```python {1,2,13}
    import arzule_ingest
    arzule_ingest.autogen.instrument_autogen()

    import os
    from autogen import AssistantAgent, UserProxyAgent

    llm_config = {"config_list": [{"model": "gpt-4", "api_key": os.environ.get("OPENAI_API_KEY")}]}
    assistant = AssistantAgent("assistant", llm_config=llm_config)
    user_proxy = UserProxyAgent("user_proxy", code_execution_config=False)

    # Traces are captured automatically
    user_proxy.initiate_chat(assistant, message="Tell me a joke about AI.")
    arzule_ingest.shutdown()
    ```
  </Tab> */}
</Tabs>

<Tip>
**About `shutdown()`**: Call `arzule_ingest.shutdown()` at the end of your script to ensure all events are flushed to Arzule. While `shutdown()` is registered with `atexit` and runs automatically on normal process exit, calling it explicitly guarantees events are sent before the script continues - especially important if your process might exit abnormally or you're running multiple agents in one script.
</Tip>

## Step 4: View your traces

Open your [Arzule dashboard](https://app.arzule.com/dashboard/overview) to see the captured traces. You'll see:

- The complete execution timeline
- Each agent's actions and decisions
- Tool calls with inputs and outputs
- LLM requests and responses

## Local development

During development, write traces to a local file instead of the cloud:

```python
from arzule_ingest import ArzuleRun
from arzule_ingest.sinks import JsonlFileSink

sink = JsonlFileSink("traces/dev.jsonl")

with ArzuleRun(tenant_id="local", project_id="dev", sink=sink) as run:
    # Your agent code here
    pass
```

View traces with the CLI:

```bash
arzule view traces/dev.jsonl
```

## Next steps

<CardGroup cols={2}>

<Card title="Claude Code Integration" icon="terminal" href="/integrations/claude-code">
  Full Claude Code instrumentation guide.
</Card>

<Card title="CrewAI Integration" icon="users" href="/integrations/crewai">
  Full CrewAI instrumentation guide.
</Card>

<Card title="LangGraph Integration" icon="diagram-project" href="/integrations/langgraph">
  Full LangGraph instrumentation guide.
</Card>

{/* <Card title="LangChain Integration" icon="link" href="/integrations/langchain">
  Full LangChain instrumentation guide.
</Card>

<Card title="AutoGen Integration" icon="robot" href="/integrations/autogen">
  Full AutoGen instrumentation guide.
</Card> */}

</CardGroup>
