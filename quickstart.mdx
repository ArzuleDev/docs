---
title: "Quickstart"
description: "Get traces flowing from your AI agents in under 5 minutes"
---

This guide walks you through installing the Arzule SDK and capturing your first traces from your agent application.

## Prerequisites

- Python 3.10 or higher
- An agent framework (CrewAI, LangChain, or AutoGen)
- An Arzule account with API credentials

## Step 1: Install the SDK

<CodeGroup>
```bash pip
pip install arzule-ingest
```

```bash poetry
poetry add arzule-ingest
```

```bash uv
uv add arzule-ingest
```
</CodeGroup>

<Note>
This installs support for CrewAI, LangChain, and AutoGen out of the box.
</Note>

## Step 2: Set environment variables

Configure your Arzule credentials. You can find these in your [Arzule dashboard](https://app.arzule.com).

```bash
export ARZULE_API_KEY="your-api-key"
export ARZULE_TENANT_ID="your-tenant-id"
export ARZULE_PROJECT_ID="your-project-id"
```

## Step 3: Initialize and run

<Tabs>
  <Tab title="CrewAI">
    Add a single line to your application startup:

    ```python
    import arzule_ingest
    from crewai import Crew, Agent, Task

    # Initialize with environment variables
    arzule_ingest.init()

    # Your existing CrewAI code works unchanged
    researcher = Agent(
        role="Researcher",
        goal="Find accurate information",
        backstory="You are a skilled researcher."
    )

    task = Task(
        description="Research the latest developments in AI safety",
        expected_output="A summary of key findings",
        agent=researcher
    )

    crew = Crew(agents=[researcher], tasks=[task])

    # Run the crew - traces are captured automatically
    result = crew.kickoff()
    ```
  </Tab>
  <Tab title="LangChain">
    Get the callback handler and pass it to your chain:

    ```python
    from arzule_ingest.langchain import instrument_langchain
    from arzule_ingest import ArzuleRun
    from arzule_ingest.sinks import HttpBatchSink
    from langchain_openai import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate

    # Get the callback handler
    handler = instrument_langchain()

    # Create sink with environment variables
    sink = HttpBatchSink()

    # Your LangChain code
    llm = ChatOpenAI(model="gpt-4")
    prompt = ChatPromptTemplate.from_template("Tell me about {topic}")
    chain = prompt | llm

    # Run inside ArzuleRun context with the handler
    with ArzuleRun(sink=sink) as run:
        result = chain.invoke(
            {"topic": "AI safety"},
            config={"callbacks": [handler]}
        )
    ```
  </Tab>
  <Tab title="AutoGen">
    Call instrument once at startup:

    ```python
    from arzule_ingest.autogen import instrument_autogen
    from arzule_ingest import ArzuleRun
    from arzule_ingest.sinks import HttpBatchSink
    from autogen import AssistantAgent, UserProxyAgent

    # Instrument AutoGen (call once at startup)
    instrument_autogen()

    # Create sink with environment variables
    sink = HttpBatchSink()

    # Your AutoGen code
    assistant = AssistantAgent(
        name="assistant",
        llm_config={"model": "gpt-4"}
    )

    user_proxy = UserProxyAgent(
        name="user_proxy",
        human_input_mode="NEVER"
    )

    # Run inside ArzuleRun context
    with ArzuleRun(sink=sink) as run:
        user_proxy.initiate_chat(
            assistant,
            message="What are the benefits of multi-agent systems?"
        )
    ```
  </Tab>
</Tabs>

## Step 4: View your traces

Open your [Arzule dashboard](https://app.arzule.com) to see the captured traces. You'll see:

- The complete execution timeline
- Each agent's actions and decisions
- Tool calls with inputs and outputs
- LLM requests and responses

## Local development

During development, write traces to a local file instead of the cloud:

```python
from arzule_ingest import ArzuleRun
from arzule_ingest.sinks import JsonlFileSink

sink = JsonlFileSink("traces/dev.jsonl")

with ArzuleRun(tenant_id="local", project_id="dev", sink=sink) as run:
    # Your agent code here
    pass
```

View traces with the CLI:

```bash
arzule view traces/dev.jsonl
```

## Next steps

<CardGroup cols={2}>

<Card title="CrewAI Integration" icon="users" href="/integrations/crewai">
  Full CrewAI instrumentation guide.
</Card>

<Card title="LangChain Integration" icon="link" href="/integrations/langchain">
  Full LangChain instrumentation guide.
</Card>

<Card title="AutoGen Integration" icon="robot" href="/integrations/autogen">
  Full AutoGen instrumentation guide.
</Card>

<Card title="Configure PII redaction" icon="shield" href="/configuration/pii-redaction">
  Control what sensitive data gets filtered.
</Card>

</CardGroup>
