---
title: "LangChain Integration"
description: "Automatic instrumentation for LangChain chains, agents, and LLM applications"
---

Arzule provides seamless integration with LangChain. Capture chain executions, LLM calls, tool invocations, and agent actions automatically.

## Installation

```bash
pip install arzule-ingest langchain-core
```

## Quick setup

Add two lines to instrument your LangChain application:

```python {1,2}
import arzule_ingest
arzule_ingest.init()

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

llm = ChatOpenAI(model="gpt-4")
prompt = ChatPromptTemplate.from_template("Tell me about {topic}")
chain = prompt | llm

# Traces are captured automatically
result = chain.invoke({"topic": "AI observability"})
```

## What gets captured

The LangChain integration automatically captures:

### Chain execution
- `chain.start` - Chain begins with inputs
- `chain.end` - Chain completes with outputs
- `chain.error` - Chain failures with error details

### LLM interactions
- `llm.call.start` - Prompts sent to the model
- `llm.call.end` - Responses with token usage
- Model name and configuration

### Tool usage
- `tool.call.start` - Tool invocation with inputs
- `tool.call.end` - Tool results or errors

### Agent actions
- `agent.action` - Agent decides to take an action
- `agent.finish` - Agent completes reasoning

### Retriever operations
- `retriever.start` - Document retrieval begins
- `retriever.end` - Documents retrieved

## Advanced configuration

### Explicit instrumentation

For fine-grained control:

```python
from arzule_ingest import ArzuleRun
from arzule_ingest.sinks import JsonlFileSink
from arzule_ingest.langchain import instrument_langchain

# Get the callback handler
handler = instrument_langchain()

sink = JsonlFileSink("traces/output.jsonl")

with ArzuleRun(
    tenant_id="your-tenant-id",
    project_id="your-project-id",
    sink=sink
) as run:
    # Pass handler to invoke
    result = chain.invoke(
        {"topic": "AI"},
        config={"callbacks": [handler]}
    )
```

### Instrumentation modes

```python
# Full instrumentation (default)
handler = instrument_langchain(mode="global")

# Minimal - only essential events
handler = instrument_langchain(mode="minimal")
```

## LangGraph

For graph-based multi-agent workflows, see the dedicated [LangGraph Integration](/integrations/langgraph) guide which covers:

- Graph execution tracking
- Node-level observability
- Multi-agent handoff detection
- Parallel execution tracking
- Checkpoint operations

<Card title="LangGraph Integration" icon="diagram-project" href="/integrations/langgraph">
  Deep observability for LangGraph applications.
</Card>

## Next steps

<CardGroup cols={2}>

<Card title="LangGraph" icon="diagram-project" href="/integrations/langgraph">
  Graph-based multi-agent workflows.
</Card>

<Card title="Quickstart" icon="rocket" href="/quickstart">
  Complete getting started guide.
</Card>

</CardGroup>
